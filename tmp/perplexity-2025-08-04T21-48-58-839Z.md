The latest developments in AI safety research in 2024 include significant advances in technical research, increased international collaboration, and the expansion of national and global institutional frameworks dedicated to AI safety.

Key research and technical trends:
- Researchers explored the dual-use risks of large language models (LLMs) in scientific domains, including safety benchmarking for chemistry applications and cross-disciplinary LLM alignment[1].
- Novel approaches to risk mitigation emerged beyond the standard reinforcement learning from human feedback (RLHF), such as "unlearning" (intentionally removing learned behaviors or knowledge from models) and developing models that are resistant to malicious fine-tuning and jailbreaks[1].
- Enhanced interpretability tools were applied for safety monitoring, enabling researchers to probe "inner representations," intermediate hidden states, and identify specific safety-relevant neurons and attention heads[1].

Institutional and governance developments:
- In 2024, several countries established or bolstered national AI safety institutes (e.g., U.S., UK, Canada, Japan), focusing on standards for safe AI development, red-teaming exercises, synthetic content detection, and authenticating AI-generated material[3]. The U.S. AI Safety Institute (AISI), launched in 2024, was particularly active in developing testing environments and collaborating with companies to identify and mitigate vulnerabilities in advanced models[3].
- The International Network of AI Safety Institutes was formed in November 2024, comprising Australia, Canada, the EU, France, Japan, Kenya, Korea, Singapore, UK, and U.S[5]. This marks the first major multilateral initiative to coordinate technical and governance standards for AI safety. Early funding—over $11 million—has been committed by the U.S., South Korea, and Australia (among others), with emphasis on synthetic content detection and risk mitigation[5].

Other noteworthy shifts and outlook:
- There is a growing trend toward research on the safety of autonomous AI agents and embodied AI (AI interacting with the real world), with alignment, robustness (especially jailbreaking), and scientific applications highlighted as rapidly evolving fronts[1].
- While most institutes focus on domestic concerns, a shift toward international coordination is apparent, with global summits and collaborative safety tests for advanced AI models in progress[3][5].
- The field witnessed both encouraging technical progress and the identification of new failure modes, sparking debate within the community on the best governance and research approaches[2].

In summary, 2024 saw progress in technical AI safety research—including interpretability, defense against misuse, and model unlearning—alongside the formation of new governance bodies and increased international cooperation, especially targeting the risks of advanced LLMs and synthetic media generation[1][3][5].

## Sources:
1. https://aisafetychina.substack.com/p/ai-safety-in-china-2024-in-review
2. https://80000hours.org/2025/01/what-happened-with-ai-2024/
3. https://www.omfif.org/2024/06/national-approaches-to-ai-safety-diverge-in-focus/
4. https://arxiv.org/html/2410.18114v3
5. https://www.siia.net/inaugural-convening-of-international-network-of-ai-safety-institutes-kicks-off-in-san-francisco/
