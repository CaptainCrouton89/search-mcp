# arXiv Search Results

**Query:** machine learning transformers
**Search Type:** General Search
**Total Results:** 528410
**Showing:** 1-3 of 528410

## 1. Aspects of Artificial Intelligence: Transforming Machine Learning Systems Naturally

**arXiv ID:** 2502.01708v1
**Authors:** Xiuzhan Guo
**Primary Category:** cs.LG
**All Categories:** cs.LG, cs.AI, cs.DB, cs.DM
**Published:** 2/3/2025

**Abstract:**
In this paper, we study the machine learning elements which we are interested in together as a machine learning system, consisting of a collection of machine learning elements and a collection of relations between the elements. The relations we concern are algebraic operations, binary relations, and binary relations with composition that can be reasoned categorically. A machine learning system transformation between two systems is a map between the systems, which preserves the relations we concern. The system transformations given by quotient or clustering, representable functor, and Yoneda embedding are highlighted and discussed by machine learning examples. An adjunction between machine learning systems, a special machine learning system transformation loop, provides the optimal way of solving problems. Machine learning system transformations are linked and compared by their maps at 2-cell, natural transformations. New insights and structures can be obtained from universal properties and algebraic structures given by monads, which are generated from adjunctions.

**Links:**
- [Abstract](http://arxiv.org/abs/2502.01708v1)
- [PDF](http://arxiv.org/pdf/2502.01708v1)

---

## 2. Efficient Private Machine Learning by Differentiable Random Transformations

**arXiv ID:** 2008.07758v1
**Authors:** Fei Zheng
**Primary Category:** cs.CR
**All Categories:** cs.CR, cs.LG, stat.ML
**Published:** 8/17/2020

**Abstract:**
With the increasing demands for privacy protection, many privacy-preserving machine learning systems were proposed in recent years. However, most of them cannot be put into production due to their slow training and inference speed caused by the heavy cost of homomorphic encryption and secure multiparty computation(MPC) methods. To circumvent this, I proposed a privacy definition which is suitable for large amount of data in machine learning tasks. Based on that, I showed that random transformations like linear transformation and random permutation can well protect privacy. Merging random transformations and arithmetic sharing together, I designed a framework for private machine learning with high efficiency and low computation cost.

**Links:**
- [Abstract](http://arxiv.org/abs/2008.07758v1)
- [PDF](http://arxiv.org/pdf/2008.07758v1)

---

## 3. Explainable Machine Learning based Transform Coding for High Efficiency Intra Prediction

**arXiv ID:** 2012.11152v1
**Authors:** Na Li, Yun Zhang, C. -C. Jay Kuo
**Primary Category:** eess.IV
**Published:** 12/20/2020

**Abstract:**
Machine learning techniques provide a chance to explore the coding performance potential of transform. In this work, we propose an explainable transform based intra video coding to improve the coding efficiency. Firstly, we model machine learning based transform design as an optimization problem of maximizing the energy compaction or decorrelation capability. The explainable machine learning based transform, i.e., Subspace Approximation with Adjusted Bias (Saab) transform, is analyzed and compared with the mainstream Discrete Cosine Transform (DCT) on their energy compaction and decorrelation capabilities. Secondly, we propose a Saab transform based intra video coding framework with off-line Saab transform learning. Meanwhile, intra mode dependent Saab transform is developed. Then, Rate Distortion (RD) gain of Saab transform based intra video coding is theoretically and experimentally analyzed in detail. Finally, three strategies on integrating the Saab transform and DCT in intra video coding are developed to improve the coding efficiency. Experimental results demonstrate that the proposed 8$\times$8 Saab transform based intra video coding can achieve Bj{\o}nteggard Delta Bit Rate (BDBR) from -1.19% to -10.00% and -3.07% on average as compared with the mainstream 8$\times$8 DCT based coding scheme.

**Comment:** 13 pages, 9 figures
**Links:**
- [Abstract](http://arxiv.org/abs/2012.11152v1)
- [PDF](http://arxiv.org/pdf/2012.11152v1)

---

